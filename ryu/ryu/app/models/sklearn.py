import os 

#要驗證的模型
valid_modelpath='./models/b255v6 RandomForest choice_random=0.004 train_size=0.8 test_size=0.2 choice_split=3 choice_train=2 1630563027.216647.pkl'
#b255v6 RandomForest choice_random=250 train_size=0.8 test_size=0.2 choice_split=3 choice_train=2 1630471952.778232.pkl
#b255v6 RandomForest choice_random=0.004 train_size=0.8 test_size=0.2 choice_split=3 choice_train=2 1630563027.216647.pkl

#輸入輸出目錄
choice_dataset='b255v6'
outputpath = './'
"""
#記得改路徑字串，該資料夾所有，*.parquet或*.csv
"""
def dataset_path(choice_dataset):
    if choice_dataset=='b255':
        trainpath = 'D:/preprocess_data/b255/*.parquet'
        testpath = 'D:/preprocess_data/b255/*.parquet'
    elif choice_dataset=='b255v6':
        trainpath = 'D:/preprocess_data/b255v6/*.parquet'
        testpath = 'D:/preprocess_data/b255v6/*.parquet'
    elif choice_dataset=='500p':
        trainpath = 'D:/preprocess_data/vpn_500size/*.csv'
        testpath = 'D:/preprocess_data/vpn_500size/*.csv'
    elif choice_dataset=='100p':
        trainpath = 'D:/preprocess_data/vpn_100size/*.csv'
        testpath = 'D:/preprocess_data/vpn_100size/*.csv'    
    elif choice_dataset=='50p':
        trainpath = 'D:/preprocess_data/vpn_50size/*.csv'
        testpath = 'D:/preprocess_data/vpn_50size/*.csv'
    elif choice_dataset=='40':
        trainpath = 'D:/preprocess_data/40/*.parquet'
        testpath = 'D:/preprocess_data/40/*.parquet'
    elif choice_dataset=='valid':
        trainpath = 'D:/preprocess_data/valid/*.parquet'
        testpath = 'D:/preprocess_data/valid/*.parquet'
    elif choice_dataset=='fb':
        trainpath = 'D:/preprocess_data/fb/*.parquet'
        testpath = 'D:/preprocess_data/fb/*.parquet'
    else:
        print("choice_dataset gg")

    return trainpath,testpath
trainpath,testpath=dataset_path(choice_dataset)

#choice_random每個pcap取幾個或取比例,注意0是比例全部(1是1個非比例1),b255一個檔案10,000,全部11,000,000記憶體可能GG
#pdn是最低取的數量,replace是數量不足能不能重複取
choice_random = 0.004
pdn_threshold = 100
randomreplace = 'False'
#size指的是取的比例,1跟0是原地考照
train_size = 0.8
test_size = 0.2

#1原地考照
#2train_test_split same rate "all class"
#3RandomUnderSampler resample to "min(all class)"
#4StratifiedShuffleSplit same rate "each class"
choice_split = 3

#1原地考照
#2同train, test
#3不同train, test
choice_train = 2

#控制要不要執行時候印
show_ctrl = 0
#要不要標題，不然存一堆會不知道誰是誰
title_ctrl = 0
#要不要計算跟存混淆矩陣跟決策樹(要是樹的才能產生)
cal_confusion_matrix = 1
cal_tree_structure = 0

#換分類改這個
# 1clf
# 2forest
# 3svc
# 4c45clf
# 5clfe
# 6lgb
# 9valid
choice_classfier = 9


"""
##################################################################################
"""


import numpy as np
from matplotlib import pyplot as plt

#處理parquet讀跟選column
import pandas as pd
#畫結果heatmap
import seaborn as sns
import itertools
#用來mergeparquet
import glob as glob
#轉datatype csv的list字串變float
from ast import literal_eval
import ast

from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score
from sklearn.datasets import load_digits
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn import tree, ensemble
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction import DictVectorizer

#LightGBM
import lightgbm as lgb

#decision tree c4.5
from c45RaczeQ.c45 import C45

#畫decision tree
import pydotplus
from sklearn.tree import export_graphviz

#存model
import joblib

#平衡取data
from imblearn.under_sampling import RandomUnderSampler

#weka
import os
import traceback


"""
#算照比例每個是多少個
#RandomUnderSampler resample to "min(all class)"
"""
def stratified_split(y, train_size):        
    def split_class(y, label, train_size):
        indices = np.flatnonzero(y == label)
        n_train = int(indices.size*train_size)
        train_index = indices[:n_train]
        test_index = indices[n_train:]
        return (train_index, test_index)
    idx = [split_class(y, label, train_size) for label in np.unique(y)]
    train_index = np.concatenate([train for train, _ in idx])
    test_index = np.concatenate([test for _, test in idx])
    return train_index, test_index

"""
dataset主要用的class
"""
class sklearn_class:
    def __init__ (self, trainpath, testpath):
        self.trainpath = trainpath
        self.testpath = testpath
        self.train_glob_merged_data = []
        self.test_glob_merged_data = []

                
    def input_func(self,funcid,fileid):
        #merge多個
        if funcid == 'train':
            glob_files = glob.glob(self.trainpath)
        elif funcid == 'test':
            glob_files = glob.glob(self.testpath)
        else:
            print('funcid gg')

        glob_merged_data = []
        glob_data = []
        for f in glob_files:
            temp = []
            if fileid == 'csv':
                temp = pd.read_csv(f, engine = 'c')
            elif fileid == 'parquet':
                temp = pd.read_parquet(f, engine = 'fastparquet')
            else:
                print('fileid gg')    
            if not choice_random == 0:
                len_index = len(temp.index)
                if isinstance(choice_random, int):
                    pdn = int(choice_random)
                    if pdn > len_index:
                        pdn = len_index
                else:
                    pdn = int(len_index*choice_random)
                if pdn-pdn_threshold <= 0:
                    pdn = pdn_threshold
                temp = temp.sample(n = pdn, replace = randomreplace)         
            glob_data.append(temp) 
            glob_merged_data = pd.concat(glob_data , ignore_index = True)

        if funcid == 'train':
            self.train_glob_merged_data = glob_merged_data
        elif funcid == 'test':
            self.test_glob_merged_data = glob_merged_data
        else:
            print('funcid gg')   
        #可印整個大檔會簡略顯示
        #print(glob_merged_data) 

    
    def input_train_csv(self):
        #merge多個csv        
        self.input_func(funcid = 'train',fileid = 'csv')
    
    def input_train_parquet(self):
        #merge多個parquet        
        self.input_func(funcid = 'train',fileid = 'parquet')
    
    def input_test_csv(self):
        self.input_func(funcid = 'test',fileid = 'csv')

    def input_test_parquet(self):
        self.input_func(funcid = 'test',fileid = 'parquet')
        

    
    def numpy_func(self,funcid,fileid):
        #轉到numpy因為sklearn要用
        if funcid == 'train':
            glob_merged_data = self.train_glob_merged_data
        elif funcid == 'test':
            glob_merged_data = self.test_glob_merged_data
        else:
            print('funcid gg')

        #for weka
        from arff import arff
        #self.weka_output_name='D:/preprocess_data/40'+'\\'+'train.arff'
        #arff.toARFF(glob_merged_data, self.weka_output_name)

        if fileid == 'csv':
            df = glob_merged_data['feature'].values
            X = [ast.literal_eval(j) for j in df]    
        elif fileid == 'parquet':
            X = glob_merged_data['feature'].values.reshape(-1,).tolist()
        else:
            print('fileid gg')   
        X = np.array(X)
        y = glob_merged_data['app_label'].values.reshape(-1,).tolist()
        y = np.array(y)
        #沒事別印會印到細節GG
        #print(X, y)

        if choice_split == 1:
            #all
            X_train = X
            y_train = y
            X_test = X
            y_test = y
        elif choice_split == 2:
            #train_test_split same rate "all class"
            X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = train_size, test_size = test_size)
        elif choice_split == 3:
            #RandomUnderSampler resample to "min(all class)"
            under_X_train, under_X_test, under_y_train, under_y_test = train_test_split(X, y, train_size = train_size, test_size = test_size)
            rus = RandomUnderSampler(random_state = 0)
            X_train, y_train = rus.fit_resample(under_X_train,under_y_train)
            X_test, y_test = rus.fit_resample(under_X_test, under_y_test)
        elif choice_split == 4:
            #StratifiedShuffleSplit same rate "each class"
            train_index, test_index = stratified_split(y, train_size)
            X_train = X[train_index]
            y_train = y[train_index]
            X_test = X[test_index]
            y_test = y[test_index]
        else:
            print('choice_split gg') 


        if funcid == 'train':
            self.train_X_train = X_train
            self.train_y_train = y_train
            self.train_X_test = X_test
            self.train_y_test = y_test
        elif funcid == 'test':
            self.test_X_train = X_train
            self.test_y_train = y_train
            self.test_X_test = X_test
            self.test_y_test = y_test
        else:
            print('funcid gg')   

    def train_numpy_csv(self):
        #trainset多個csv  
        self.numpy_func(funcid = 'train',fileid = 'csv')

    def train_numpy_parquet(self):
        #trainset多個parquet
        self.numpy_func(funcid = 'train',fileid = 'parquet')    
        
    def test_numpy_csv(self):
        self.numpy_func(funcid = 'test',fileid = 'csv')

    def test_numpy_parquet(self):
        self.numpy_func(funcid = 'test',fileid = 'parquet')
        

"""
#classifier
"""
def classifier_clf(X_train, y_train, X_test):
    #Decision Tree
    classifier = 'DecisionTree'
    clf = DecisionTreeClassifier(max_leaf_nodes = 128, random_state = 0)
    clf.fit(X_train, y_train)
    print_tree(clf,classifier)
    y_test_predicted = clf.predict(X_test)
    save_models(clf,classifier)
    return y_test_predicted, classifier


def classifier_forest(X_train, y_train, X_test):
    #random forest
    classifier = 'RandomForest'
    forest = ensemble.RandomForestClassifier(n_estimators = 500, criterion="entropy", class_weight="balanced" )
    forest.fit(X_train, y_train)
    y_test_predicted = forest.predict(X_test)
    save_models(forest,classifier)
    return y_test_predicted, classifier


def classifier_svc(X_train, y_train, X_test):
    #svm 
    classifier = 'SupportVectorMachine'   
    svc = SVC(random_state = 0)
    svc.fit(X_train, y_train)
    y_test_predicted = svc.predict(X_test)
    save_models(svc,classifier)
    return y_test_predicted, classifier


def classifier_c45clf(X_train, y_train, X_test):
    #c4.5 
    classifier = 'C4.5'
    c45clf = C45()
    c45clf.fit(X_train, y_train)
    #print_tree(c45clf,classifier)
    y_test_predicted = c45clf.predict(X_test)
    save_models(c45clf,classifier)
    return y_test_predicted, classifier
 

def classifier_clfe(X_train, y_train, X_test):
    #Decision Tree entropy
    classifier = 'DecisionTreeEntropy'
    clfe = DecisionTreeClassifier(criterion = 'entropy',max_leaf_nodes = 64, random_state = 0)
    clfe.fit(X_train, y_train)
    #print_tree(clfe)
    y_test_predicted = clfe.predict(X_test)
    save_models(clfe,classifier)
    return y_test_predicted, classifier


def classifier_valid(readclf, X_test):
    #validation
    classifier = 'validation'
    clf = readclf
    y_test_predicted = clf.predict(X_test)
    return y_test_predicted, classifier


def classifier_lgb(X_train, y_train, X_test):
    #Decision Tree
    classifier = 'LightGBM'
    params_sklearn = {
    'learning_rate':0.1,
    'max_bin':64,
    'num_leaves':128,    
    'max_depth':16,
    
    'reg_alpha':0.1,
    'reg_lambda':0.2,   
     
    'objective':'multiclass',
    'n_estimators':512,
    }
    
    gbm = lgb.LGBMClassifier(**params_sklearn)
    gbm.fit(X_train, y_train)
    y_test_predicted = gbm.predict(X_test)
    save_models(gbm,classifier)
    return y_test_predicted, classifier





"""
result
"""
def file_timestamp():
    from datetime import datetime, timezone, timedelta
    rt = timezone(timedelta(hours =+8))
    titlet = str(datetime.now().timestamp())
    patht = datetime.now(rt).isoformat(timespec = "seconds")
    return titlet,patht


def print_result(test_y_test, y_test_predicted, classifier):
    ID_TO_APP = {
    0: 'AIM Chat', 
    1: 'Email', 
    2: 'Facebook', 
    3: 'FTPS', 
    4: 'Gmail', 
    5: 'Hangouts', 
    6: 'ICQ', 
    7: 'Netflix', 
    8: 'SCP', 
    9: 'SFTP', 
    10: 'Skype', 
    11: 'Spotify', 
    12: 'Torrent', 
    13: 'Tor', 
    14: 'Vimeo', 
    15: 'Voipbuster', 
    16: 'Youtube', 
    }
    labelslist=[]
    targetnameslist=[]
    for k,v in ID_TO_APP.items():
        labelslist.append(k)
        targetnameslist.append(v)

    #result
    #print(test_y_test)
    patht,titlet = file_timestamp()
    result_title = classifier+'\n'+trainpath+'\n'+titlet
    png_title = patht

    #colormaps cmap=
    #https://matplotlib.org/stable/tutorials/colors/colormaps.html
    #confusion_matrix    
    if cal_confusion_matrix == 1:
        print(confusion_matrix(test_y_test, y_test_predicted,labels=labelslist))
        cm_report = confusion_matrix(test_y_test, y_test_predicted,labels=labelslist, normalize = None)
        sns.set(font_scale=0.35)
        sns.heatmap(pd.DataFrame(cm_report).iloc[:,:].T, annot = True, fmt=".4g", cmap = "Blues")
        pngoutput_path = os.path.join(outputpath+'/results/'+ png_title+'.'+'confusion_matrix'+'.'+classifier +'.png')
        if title_ctrl == 1:
            plt.title(result_title)
        plt.savefig(pngoutput_path,dpi = 300)
        if show_ctrl == 1:
            plt.show()
        plt.clf()
    
    #classification_report
    print(classification_report(test_y_test, y_test_predicted,labels=labelslist,target_names=targetnameslist, digits = 4))
    clf_report = classification_report(test_y_test, y_test_predicted,labels=labelslist,target_names=targetnameslist, digits = 4, output_dict = True)
    mask=pd.DataFrame(clf_report).iloc[:,:].T

    sns.set(font_scale=0.5)
    #nocolor[row,column]
    mask3=mask.copy()
    mask3.iloc[:-3,:] = float('nan')
    sns.heatmap(mask3, annot = True, fmt=".4g", cmap = "binary",cbar=False)
    mask2=mask.copy()
    mask2.iloc[:,:-1] = float('nan')
    mask2.iloc[-3:,:] = float('nan')
    sns.heatmap(mask2, annot = True, fmt=".4g", cmap = "Oranges")
    mask1=mask.copy()
    mask1.iloc[:,-1] = float('nan')
    mask1.iloc[-3:,:] = float('nan')
    sns.heatmap(mask1, annot = True, fmt=".4g", cmap = "Blues")
    pngoutput_path = os.path.join(outputpath+'/results/'+ png_title+'.'+'classification_report'+'.'+'.'+classifier +'.png')
    
    if title_ctrl == 1:
        plt.title(result_title)
    plt.savefig(pngoutput_path,dpi = 300)
    if show_ctrl == 1:
        plt.show()
    plt.clf()

def save_models(clf,classifier):
    patht,titlet = file_timestamp()
    skloutput_path = os.path.join(outputpath+'/models/'+
                                    choice_dataset+' '+
                                    classifier+' '+
                                    'choice_random='+str(choice_random)+' '+
                                    'train_size='+str(train_size)+' '+
                                    'test_size='+str(test_size)+' '+
                                    'choice_split='+str(choice_split)+' '+
                                    'choice_train='+str(choice_train)+' '+
                                    patht+
                                    '.pkl')
    joblib.dump(clf,skloutput_path)

def print_tree(clf, classifier):
    headerdict={
        0:'IPv4_Version_IHL___IPv6_Version_Traffic_class1',
        1:'IPv4_DSCP_ECN___IPv6_Traffic_class2_Flow_label1',
        2:'IPv4_Total_Length1___IPv6_Flow_label2',
        3:'IPv4_Total_Length2___IPv6_Flow_label3',
        4:'IPv4_Id1___IPv6_Payload_length1',
        5:'IPv4_Id2___IPv6_Payload_length2',
        6:'IPv4_Flags_Fragment_Offset1___IPv6_Next_header',
        7:'IPv4_Fragment_Offset2___IPv6_Hop_limit',
        8:'IPv4_Time_To_Live___IPv6_src_IP_Addr1',
        9:'IPv4_Protocol___IPv6_src_IP_Addr2',
        10:'IPv4_Header_Checksum1___IPv6_src_IP_Addr3',
        11:'IPv4_Header_Checksum2___IPv6_src_IP_Addr4',
        12:'IPv4_src_IP_Addr1___IPv6_src_IP_Addr5',
        13:'IPv4_src_IP_Addr2___IPv6_src_IP_Addr6',
        14:'IPv4_src_IP_Addr3___IPv6_src_IP_Addr7',
        15:'IPv4_src_IP_Addr4___IPv6_src_IP_Addr8',
        16:'IPv4_dst_IP_Addr1___IPv6_src_IP_Addr9',
        17:'IPv4_dst_IP_Addr2___IPv6_src_IP_Addr10',
        18:'IPv4_dst_IP_Addr3___IPv6_src_IP_Addr11',
        19:'IPv4_dst_IP_Addr4___IPv6_src_IP_Addr12',
        20:'TCP_UDP_src_port1___IPv6_src_IP_Addr13',
        21:'TCP_UDP_src_port2___IPv6_src_IP_Addr14',
        22:'TCP_UDP_dst_port1___IPv6_src_IP_Addr15',
        23:'TCP_UDP_dst_port2___IPv6_src_IP_Addr16',
        24:'TCP_seqnum1_UDP_length1___IPv6_dst_IP_Addr1',
        25:'TCP_seqnum2_UDP_length2___IPv6_dst_IP_Addr2',
        26:'TCP_seqnum3_UDP_checksum1___IPv6_dst_IP_Addr3',
        27:'TCP_seqnum4_UDP_checksum2___IPv6_dst_IP_Addr4',
        28:'TCP_acknum1___IPv6_dst_IP_Addr5',
        29:'TCP_acknum2___IPv6_dst_IP_Addr6',
        30:'TCP_acknum3___IPv6_dst_IP_Addr7',
        31:'TCP_acknum4___IPv6_dst_IP_Addr8',
        32:'TCP_Data_offset_Reserved_NS___IPv6_dst_IP_Addr9',
        33:'TCP_flagbit___IPv6_dst_IP_Addr10',
        34:'TCP_Window_Size1___IPv6_dst_IP_Addr11',
        35:'TCP_Window_Size2___IPv6_dst_IP_Addr12',
        36:'TCP_Checksum1___IPv6_dst_IP_Addr13',
        37:'TCP_Checksum2___IPv6_dst_IP_Addr14',
        38:'TCP_Urgent_pointer1___IPv6_dst_IP_Addr15',
        39:'TCP_Urgent_pointer2___IPv6_dst_IP_Addr16',
        }

    if cal_tree_structure == 1:   
        """
        sklearn example Tree structure
        """
        n_nodes = clf.tree_.node_count
        children_left = clf.tree_.children_left
        children_right = clf.tree_.children_right
        feature = clf.tree_.feature
        threshold = clf.tree_.threshold

        node_depth = np.zeros(shape = n_nodes, dtype = np.int64)
        is_leaves = np.zeros(shape = n_nodes, dtype = bool)
        stack = [(0, 0)]  # start with the root node id (0) and its depth (0)
        while len(stack) > 0:
            # `pop` ensures each node is only visited once
            node_id, depth = stack.pop()
            node_depth[node_id] = depth

            # If the left and right child of a node is not the same we have a split
            # node
            is_split_node = children_left[node_id] != children_right[node_id]
            # If a split node, append left and right children and depth to `stack`
            # so we can loop through them
            if is_split_node:
                stack.append((children_left[node_id], depth + 1))
                stack.append((children_right[node_id], depth + 1))
            else:
                is_leaves[node_id] = True

        print("The binary tree structure has {n} nodes and has "
            "the following tree structure:\n".format(n = n_nodes))
        for i in range(n_nodes):
            if is_leaves[i]:
                print("{space}node = {node} is a leaf node.".format(
                    space = node_depth[i] * "\t", node = i))
            else:
                print("{space}node = {node} is a split node: "
                    "go to node {left} if X[:, {feature}] <= {threshold} "
                    "else to node {right}.".format(
                        space = node_depth[i] * "\t",
                        node = i,
                        left = children_left[i],
                        feature = feature[i],
                        threshold = threshold[i],
                        right = children_right[i]))
        """
        We can compare the above output to the plot of the decision tree.
        END example
        """

    patht,titlet = file_timestamp()
    png_title = patht
    pngoutput_path = os.path.join(outputpath+'/results/'+ png_title+'.'+'plottree'+'.'+'.'+classifier +'.png')
    result_title = titlet
    result_title = classifier+'\n'+trainpath+'\n'+result_title
    dot_data = export_graphviz(clf,
                                out_file=None,
                                feature_names=headerdict,
                                filled=True,
                                rounded=True)
    pydot_graph = pydotplus.graph_from_dot_data(dot_data)
    pydot_graph.set_size('"64,64!"')
    pydot_graph.write_png(pngoutput_path)


def main():
    if trainpath[(len('.csv')*-1):] == '.csv':
        #read
        print('read csv...', end ='')
        ttt = sklearn_class(trainpath, testpath)
        print('done')
        #train
        print('read train...', end ='')
        ttt.input_train_csv()
        ttt.train_numpy_csv()
        print('done')
        #test
        if choice_train == 3:
            print('read test...', end ='')
            ttt.input_test_csv()
            ttt.test_numpy_csv()
            print('done')

    elif trainpath[(len('.parquet')*-1):] == '.parquet' :
        #read
        print('read parquet...', end ='')
        ttt = sklearn_class(trainpath, testpath)
        print('done')
        #train
        print('read train...', end = '')
        ttt.input_train_parquet()
        ttt.train_numpy_parquet()
        print('done')
        #test
        if choice_train == 3:
            print('read test...', end ='')
            ttt.input_test_parquet()
            ttt.test_numpy_parquet()
            print('done')

    
    print('training...', end = '\n')
    if choice_train == 1:
        #原地考照
        X_train, y_train, X_test, y_test = ttt.train_X_train, ttt.train_y_train, ttt.train_X_train, ttt.train_y_train
    elif choice_train == 2:
        #同train, test
        X_train, y_train, X_test, y_test = ttt.train_X_train, ttt.train_y_train, ttt.train_X_test, ttt.train_y_test
    elif choice_train == 3:
        #不同train, test
        X_train, y_train, X_test, y_test = ttt.train_X_train, ttt.train_y_train, ttt.test_X_test, ttt.test_y_test
    else:
        print('choice_train gg')
    
    if choice_classfier == 1:
        y_test_predicted, classifier = classifier_clf(X_train, y_train, X_test)
    elif  choice_classfier == 2:
        y_test_predicted, classifier = classifier_forest(X_train, y_train, X_test)
    elif  choice_classfier == 3:
        y_test_predicted, classifier = classifier_svc(X_train, y_train, X_test)
    elif  choice_classfier == 4:
        y_test_predicted, classifier = classifier_c45clf(X_train, y_train, X_test)
    elif choice_classfier == 5:
        y_test_predicted, classifier = classifier_clfe(X_train, y_train, X_test)
    elif choice_classfier == 6:
        y_test_predicted, classifier = classifier_lgb(X_train, y_train, X_test)    
    elif choice_classfier == 9:
        #讀取Model
        import joblib
        readclf = joblib.load(valid_modelpath)
        y_test_predicted, classifier = classifier_valid(readclf, X_test)
    else:
        print('choice_classfier gg')

    if choice_classfier:
        print_result(y_test, y_test_predicted, classifier)
    print('\n', '================ We Can Only See A Short Distance Ahead. ================', '\n')  

if __name__ == '__main__':
    main()